{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World: Content-Addressed Knowledge Substrate\n",
    "\n",
    "This notebook demonstrates two substrate-level properties described in\n",
    "[*A Content-Addressed Adaptive Knowledge Substrate for Distributed Epistemic Coordination*](joven_knowledge_substrate.md) (Joven, 2026):\n",
    "\n",
    "1. **Content-addressed change detection** — if a fact node's content changes, its hash changes.\n",
    "   Unchanged nodes retain hashes and are recognizable as unchanged by hash comparison alone.\n",
    "\n",
    "2. **Scoped recomputation of derived conclusions** — a cached derivation is keyed by the hashes\n",
    "   of the facts it depends on. After a perturbation, the system invalidates only derivations\n",
    "   whose dependency hashes are no longer present, then recomputes just those.\n",
    "\n",
    "This connects to the LLM reasoning failure taxonomy in\n",
    "[Song et al. (2026)](https://arxiv.org/abs/2602.06176) — specifically:\n",
    "\n",
    "| Failure category (Song et al.) | Substrate mechanism demonstrated here |\n",
    "|-------------------------------|---------------------------------------|\n",
    "| Working memory / proactive interference | Knowledge externalized into persistent graph; updates produce new hashes, not token competition |\n",
    "| Multi-agent termination | Fixed-point convergence: cycle ends when root hash stabilizes |\n",
    "| Redundant recomputation | Tier 1 cache check avoids re-deriving conclusions whose inputs haven't changed |\n",
    "\n",
    "This is intentionally minimal. It does not claim to eliminate hallucination or solve\n",
    "compositional reasoning in general. It demonstrates deterministic invalidation and\n",
    "recomputation boundaries in a typed, content-addressed substrate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primitives\n",
    "\n",
    "Three building blocks: typed nodes, content-addressed hashing, and typed edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class NodeType(Enum):\n",
    "    ENTITY = \"entity\"\n",
    "    RELATION = \"relation\"\n",
    "    DERIVED = \"derived\"\n",
    "\n",
    "\n",
    "class EdgeType(Enum):\n",
    "    SUBJECT = \"subject\"\n",
    "    OBJECT = \"object\"\n",
    "    DEPENDS = \"depends\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"Content-addressed node. Hash is deterministic over type + content + dependency hashes.\"\"\"\n",
    "    node_type: NodeType\n",
    "    content: dict\n",
    "    dependency_hashes: tuple[str, ...] = field(default_factory=tuple)\n",
    "    _hash: Optional[str] = field(default=None, repr=False)\n",
    "\n",
    "    @property\n",
    "    def hash(self) -> str:\n",
    "        if self._hash is None:\n",
    "            payload = json.dumps(\n",
    "                {\n",
    "                    \"type\": self.node_type.value,\n",
    "                    \"content\": self.content,\n",
    "                    \"deps\": list(self.dependency_hashes),\n",
    "                },\n",
    "                sort_keys=True,\n",
    "                separators=(\",\", \":\"),\n",
    "            )\n",
    "            self._hash = hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n",
    "        return self._hash\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        label = self.content.get(\"label\", self.content)\n",
    "        return f\"Node({self.node_type.name}, {label}, {self.hash[:12]}...)\"\n",
    "\n",
    "\n",
    "print(\"Primitives loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated LLM\n",
    "\n",
    "A deterministic stand-in for an LLM that performs simple transitive reasoning\n",
    "over fact nodes. Tracks call count and token usage so we can measure\n",
    "the cost difference between full recomputation and scoped recomputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedLLM:\n",
    "    def __init__(self):\n",
    "        self.call_count = 0\n",
    "        self.total_tokens = 0\n",
    "        self.call_log: list[dict] = []\n",
    "\n",
    "    def reason_over_subgraph(self, facts: list[dict], query: str) -> dict:\n",
    "        self.call_count += 1\n",
    "\n",
    "        prompt_tokens = sum(len(json.dumps(f, separators=(\",\", \":\"))) for f in facts) + len(query)\n",
    "        completion_tokens = 60\n",
    "        total = prompt_tokens + completion_tokens\n",
    "        self.total_tokens += total\n",
    "\n",
    "        entities: dict[str, str] = {}\n",
    "        relations: list[dict] = []\n",
    "        for f in facts:\n",
    "            if f.get(\"type\") == \"entity\":\n",
    "                entities[f[\"id\"]] = f[\"name\"]\n",
    "            elif f.get(\"type\") == \"relation\":\n",
    "                relations.append(f)\n",
    "\n",
    "        relations = sorted(relations, key=lambda r: r.get(\"order\", 0))\n",
    "\n",
    "        if len(relations) >= 2:\n",
    "            r1, r2 = relations[0], relations[1]\n",
    "            conclusion = (\n",
    "                f\"By transitivity: {entities[r1['subject']]} {r1['predicate']} {entities[r1['object']]}, \"\n",
    "                f\"and {entities[r2['subject']]} {r2['predicate']} {entities[r2['object']]}. \"\n",
    "                f\"Therefore {entities[r1['subject']]} {r1['predicate']} {entities[r2['object']]}.\"\n",
    "            )\n",
    "            hops = 2\n",
    "        elif len(relations) == 1:\n",
    "            r = relations[0]\n",
    "            conclusion = f\"Direct: {entities[r['subject']]} {r['predicate']} {entities[r['object']]}.\"\n",
    "            hops = 1\n",
    "        else:\n",
    "            conclusion = \"Insufficient facts for derivation.\"\n",
    "            hops = 0\n",
    "\n",
    "        result = {\"conclusion\": conclusion, \"confidence\": 0.95, \"reasoning_hops\": hops}\n",
    "\n",
    "        self.call_log.append({\n",
    "            \"query\": query,\n",
    "            \"input_facts\": len(facts),\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": total,\n",
    "        })\n",
    "        return result\n",
    "\n",
    "\n",
    "print(\"SimulatedLLM loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substrate\n",
    "\n",
    "The substrate holds content-addressed nodes, typed edges, a root hash committing\n",
    "to the full graph state, a delta chain recording state transitions, and a\n",
    "derivation cache keyed by dependency hashes.\n",
    "\n",
    "Key operations:\n",
    "- **`commit`** — compute root hash over nodes + edges, append to delta chain\n",
    "- **`is_cached_derivation_valid`** — Tier 1 check: are all dependency hashes still present?\n",
    "- **`derive`** — Tier 3: invoke the LLM, cache the result keyed by dependency hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Substrate:\n",
    "    def __init__(self, llm: SimulatedLLM):\n",
    "        self.nodes: dict[str, Node] = {}\n",
    "        self.edges: list[tuple[str, str, str]] = []  # (src, dst, edge_type)\n",
    "        self.root_hash: Optional[str] = None\n",
    "        self.delta_chain: list[dict] = []\n",
    "        self.derived_cache: dict[str, Node] = {}\n",
    "        self.llm = llm\n",
    "\n",
    "    def add_node(self, node: Node) -> str:\n",
    "        h = node.hash\n",
    "        self.nodes.setdefault(h, node)\n",
    "        return h\n",
    "\n",
    "    def add_edge(self, src_hash: str, dst_hash: str, edge_type: EdgeType) -> None:\n",
    "        self.edges.append((src_hash, dst_hash, edge_type.value))\n",
    "\n",
    "    def remove_node(self, node_hash: str) -> None:\n",
    "        \"\"\"Remove a node and all edges referencing it.\"\"\"\n",
    "        self.nodes.pop(node_hash, None)\n",
    "        self.edges = [\n",
    "            (s, d, t) for s, d, t in self.edges\n",
    "            if s != node_hash and d != node_hash\n",
    "        ]\n",
    "\n",
    "    def compute_root_hash(self) -> str:\n",
    "        \"\"\"Root hash commits to BOTH nodes and edges — graph state, not just node multiset.\"\"\"\n",
    "        node_hashes = sorted(self.nodes.keys())\n",
    "        edges = sorted(self.edges)\n",
    "        payload = json.dumps({\"nodes\": node_hashes, \"edges\": edges}, separators=(\",\", \":\"))\n",
    "        return hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def commit(self, label: str = \"\") -> dict:\n",
    "        new_root = self.compute_root_hash()\n",
    "        delta = {\n",
    "            \"label\": label,\n",
    "            \"prev_root\": self.root_hash,\n",
    "            \"new_root\": new_root,\n",
    "            \"changed\": new_root != self.root_hash,\n",
    "            \"node_count\": len(self.nodes),\n",
    "            \"edge_count\": len(self.edges),\n",
    "        }\n",
    "        self.delta_chain.append(delta)\n",
    "        self.root_hash = new_root\n",
    "        return delta\n",
    "\n",
    "    def is_cached_derivation_valid(self, query_key: str) -> bool:\n",
    "        \"\"\"Tier 1: O(k) hash-presence check. No serialization, no model call.\"\"\"\n",
    "        cached = self.derived_cache.get(query_key)\n",
    "        if cached is None:\n",
    "            return False\n",
    "        return all(dep in self.nodes for dep in cached.dependency_hashes)\n",
    "\n",
    "    def derive(self, query: str, fact_hashes: list[str], query_key: str) -> Node:\n",
    "        \"\"\"Tier 3: invoke LLM over a subgraph, cache result keyed by dependency hashes.\"\"\"\n",
    "        facts = []\n",
    "        for h in fact_hashes:\n",
    "            node = self.nodes[h]\n",
    "            facts.append({\"type\": node.node_type.value, **node.content})\n",
    "\n",
    "        result = self.llm.reason_over_subgraph(facts, query)\n",
    "\n",
    "        derived = Node(\n",
    "            node_type=NodeType.DERIVED,\n",
    "            content={\"query\": query, \"label\": result[\"conclusion\"], **result},\n",
    "            dependency_hashes=tuple(sorted(fact_hashes)),\n",
    "        )\n",
    "        self.add_node(derived)\n",
    "        self.derived_cache[query_key] = derived\n",
    "        return derived\n",
    "\n",
    "\n",
    "print(\"Substrate loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 1: Build a knowledge graph and derive a conclusion\n",
    "\n",
    "We create a simple 2-hop management chain: Alice manages Bob, Bob manages Carol.\n",
    "Then we ask the substrate to derive who Alice transitively manages.\n",
    "\n",
    "This is the baseline: one LLM call, full cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = SimulatedLLM()\n",
    "substrate = Substrate(llm)\n",
    "\n",
    "# Entities\n",
    "alice = Node(NodeType.ENTITY, {\"id\": \"alice\", \"name\": \"Alice\", \"label\": \"Alice\", \"type\": \"entity\"})\n",
    "bob   = Node(NodeType.ENTITY, {\"id\": \"bob\",   \"name\": \"Bob\",   \"label\": \"Bob\",   \"type\": \"entity\"})\n",
    "carol = Node(NodeType.ENTITY, {\"id\": \"carol\", \"name\": \"Carol\", \"label\": \"Carol\", \"type\": \"entity\"})\n",
    "\n",
    "alice_h = substrate.add_node(alice)\n",
    "bob_h   = substrate.add_node(bob)\n",
    "carol_h = substrate.add_node(carol)\n",
    "\n",
    "# Relations\n",
    "rel1 = Node(NodeType.RELATION, {\n",
    "    \"id\": \"r1\", \"label\": \"Alice manages Bob\",\n",
    "    \"subject\": \"alice\", \"object\": \"bob\",\n",
    "    \"predicate\": \"manages\", \"order\": 0, \"type\": \"relation\",\n",
    "})\n",
    "rel2 = Node(NodeType.RELATION, {\n",
    "    \"id\": \"r2\", \"label\": \"Bob manages Carol\",\n",
    "    \"subject\": \"bob\", \"object\": \"carol\",\n",
    "    \"predicate\": \"manages\", \"order\": 1, \"type\": \"relation\",\n",
    "})\n",
    "\n",
    "rel1_h = substrate.add_node(rel1)\n",
    "rel2_h = substrate.add_node(rel2)\n",
    "\n",
    "# Edges\n",
    "substrate.add_edge(rel1_h, alice_h, EdgeType.SUBJECT)\n",
    "substrate.add_edge(rel1_h, bob_h, EdgeType.OBJECT)\n",
    "substrate.add_edge(rel2_h, bob_h, EdgeType.SUBJECT)\n",
    "substrate.add_edge(rel2_h, carol_h, EdgeType.OBJECT)\n",
    "\n",
    "delta = substrate.commit(\"initial\")\n",
    "\n",
    "print(f\"Nodes: {len(substrate.nodes)}  Edges: {len(substrate.edges)}\")\n",
    "print(f\"Root:  {substrate.root_hash[:16]}...\")\n",
    "print()\n",
    "for h, n in substrate.nodes.items():\n",
    "    print(f\"  {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive a transitive conclusion (Tier 3 — LLM call)\n",
    "query = \"Who does Alice transitively manage through the chain of command?\"\n",
    "query_key = \"transitive_management\"\n",
    "all_fact_hashes = [alice_h, bob_h, carol_h, rel1_h, rel2_h]\n",
    "\n",
    "derived1 = substrate.derive(query, all_fact_hashes, query_key)\n",
    "substrate.commit(\"derived_1\")\n",
    "\n",
    "phase1_calls = llm.call_count\n",
    "phase1_tokens = llm.total_tokens\n",
    "\n",
    "print(f\"Conclusion: {derived1.content['conclusion']}\")\n",
    "print(f\"LLM calls:  {phase1_calls}\")\n",
    "print(f\"Tokens:     {phase1_tokens}\")\n",
    "print(f\"Root:       {substrate.root_hash[:16]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: Perturb one fact, observe scoped invalidation\n",
    "\n",
    "We replace Carol with Dave in hop-2 of the management chain.\n",
    "\n",
    "The substrate detects that the cached derivation's dependency hashes no longer\n",
    "all resolve — `carol_h` and `rel2_h` are gone. This is a **Tier 1** check:\n",
    "O(k) hash lookups, no serialization, no model call.\n",
    "\n",
    "Only after invalidation does the system invoke a **Tier 3** recomputation\n",
    "over the updated subgraph.\n",
    "\n",
    "This maps to the preprint's tiered operation cost model (§2.3):\n",
    "- Tier 1 (trivial): hash-presence check → flags the stale derivation\n",
    "- Tier 3 (expensive): LLM call → runs only because Tier 1 confirmed it's needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old hop-2 (node + edges cleaned up together)\n",
    "substrate.remove_node(rel2_h)\n",
    "substrate.remove_node(carol_h)\n",
    "\n",
    "# Add new hop-2\n",
    "dave = Node(NodeType.ENTITY, {\"id\": \"dave\", \"name\": \"Dave\", \"label\": \"Dave\", \"type\": \"entity\"})\n",
    "dave_h = substrate.add_node(dave)\n",
    "\n",
    "rel2_new = Node(NodeType.RELATION, {\n",
    "    \"id\": \"r2\", \"label\": \"Bob manages Dave\",\n",
    "    \"subject\": \"bob\", \"object\": \"dave\",\n",
    "    \"predicate\": \"manages\", \"order\": 1, \"type\": \"relation\",\n",
    "})\n",
    "rel2_new_h = substrate.add_node(rel2_new)\n",
    "\n",
    "substrate.add_edge(rel2_new_h, bob_h, EdgeType.SUBJECT)\n",
    "substrate.add_edge(rel2_new_h, dave_h, EdgeType.OBJECT)\n",
    "\n",
    "substrate.commit(\"perturb\")\n",
    "\n",
    "print(f\"Root after perturbation: {substrate.root_hash[:16]}...\")\n",
    "print(f\"Nodes: {len(substrate.nodes)}  Edges: {len(substrate.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tier 1: check if cached derivation is still valid\n",
    "valid = substrate.is_cached_derivation_valid(query_key)\n",
    "print(f\"Cached derivation valid? {valid}\")\n",
    "\n",
    "if not valid:\n",
    "    # Tier 3: recompute only the invalidated derivation\n",
    "    fact_hashes2 = [alice_h, bob_h, dave_h, rel1_h, rel2_new_h]\n",
    "    derived2 = substrate.derive(query, fact_hashes2, query_key)\n",
    "    substrate.commit(\"derived_2\")\n",
    "    print(f\"Recomputed: {derived2.content['conclusion']}\")\n",
    "else:\n",
    "    print(\"Cache hit — no recomputation needed.\")\n",
    "\n",
    "phase2_calls = llm.call_count - phase1_calls\n",
    "phase2_tokens = llm.total_tokens - phase1_tokens\n",
    "\n",
    "print(f\"\\nPhase 2 LLM calls: {phase2_calls}\")\n",
    "print(f\"Phase 2 tokens:    {phase2_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fixed-Point Convergence\n",
    "\n",
    "From the preprint (§2.5): *\"A reasoning cycle terminates naturally when running\n",
    "another cycle would produce the same root hash.\"*\n",
    "\n",
    "After Phase 2, no new facts have arrived and the derivation cache is fresh.\n",
    "Committing again produces the same root hash — the system is at a fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_root = substrate.root_hash\n",
    "substrate.commit(\"fixed_point_check\")\n",
    "post_root = substrate.root_hash\n",
    "\n",
    "print(f\"Root before: {pre_root[:16]}...\")\n",
    "print(f\"Root after:  {post_root[:16]}...\")\n",
    "print(f\"Fixed point: {pre_root == post_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cost Summary\n",
    "\n",
    "In a naive system without content-addressing, Phase 2 would re-derive *all*\n",
    "conclusions from scratch (cold-start overhead — preprint §6). Here, the\n",
    "Tier 1 hash check costs nothing, and only the one invalidated derivation\n",
    "triggers a Tier 3 LLM call.\n",
    "\n",
    "With N cached derivations and 1 perturbation, naive cost is O(N) LLM calls.\n",
    "Substrate cost is O(k) hash lookups + O(invalidated) LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Cost Summary ===\")\n",
    "print(f\"Phase 1:  {phase1_calls} LLM call(s), {phase1_tokens} tokens\")\n",
    "print(f\"Phase 2:  {phase2_calls} LLM call(s), {phase2_tokens} tokens\")\n",
    "print(f\"Total:    {llm.call_count} LLM call(s), {llm.total_tokens} tokens\")\n",
    "print()\n",
    "print(\"=== LLM Call Log ===\")\n",
    "for i, entry in enumerate(llm.call_log):\n",
    "    print(f\"  [{i}] facts={entry['input_facts']} tokens={entry['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Delta Chain\n",
    "\n",
    "The full history of state transitions. From the preprint (§2.4):\n",
    "*\"Any state in the history is reconstructible from the delta chain;\n",
    "the substrate always knows what it knew at any prior moment.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Delta Chain ===\")\n",
    "for i, d in enumerate(substrate.delta_chain):\n",
    "    print(\n",
    "        f\"  [{i}] {d['label']:20s}  changed={str(d['changed']):5s}\"\n",
    "        f\"  nodes={d['node_count']}  edges={d['edge_count']}\"\n",
    "        f\"  root={d['new_root'][:12]}...\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}